# ISE 201 - Math Foundations for Decision and Data Science

## Types of Questions

- Descriptive: what is the summarized statistic of a dataset
- Exploratory: are there any patterns, trends or relationships within a dataset
- Inferential: what can be inferred about the population from the sample
- Predictive: what would the outcome be for a certain combination of features 
- Casual: how does changes in the levels of one factor causes changes in the other factor

## Understanding Data

|              | feature1 | feature2 | feature3 | featureN |
|--------------|----------|----------|----------|----------|
| observation1 |
| observation2 |
| observation3 |
| observation4 |

**features**: independent variable "x"

- **continous**: feature values that can take on **any** values within a range
    - compute summary statistics (mean, median, mode, variance, range, min, max) 
    - usable in mathematical transformations
- **categorical**: feature values represented in labels
    - compute summary statistics by categories
    - **nominal**: no order (`dog, cat, fish, etc...`)
    - **ordinal**: yes order (`high school < bachelor's < Master's < PhD`)

Continuous variables can be converted to categorical

**central tendency**: measure that identifies the center/typical value of a data set

- **arthmetic mean**: $\bar{x} = \frac{1}{n} \sum_{i=1}^{n} x_i$
- **median**: middle score or average of two middle scores when data is ranked
- **mode**: most frequent

**spread**: how data values are distributed

- **range**:$ \text{range} = \max(x) - \min(x) $
- **inter quartile range (IQR)**: $ IQR = \textit{UpperQuartile} - \textit{LowerQuartile} $
- **population variance**: 
$$ \sigma^2 = \frac{\sum_{i=1}^{n} (x_i - \mu)^2}{n} $$
- **population standard deviation**: 
$$ \sigma = \sqrt{\frac{\sum_{i=1}^{n} (x_i - \mu)^2}{n}} $$
- **sample variance**:
$$ s^2 = \frac{\sum_{i=1}^{n} (x_i - \bar{x})^2}{n - 1} $$

Population variance is for analyzing entire dataset and sample variance is for the subset

**relationship**: the amount of influence one variable change has on another

- **covariance**: 
    - positive = variables increase together
    - negative = one variable increase = other variable decrease
    - provides only the relationship direction
$$cov(X, Y) = \frac{\sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})}{n-1}$$
- **correlation**:
    - measure both strength + direction
    - values between 1 and -1
    - 1 = perfect positive linear relationship
    - -1 = perfect negative linear relationship
    - 0 = no relationship
$$p =\frac{cov_{x,y}}{{s_xs_y}}$$

## Factorizations

**elimination** : reducing a square matrix $A$ to lower $L$ and upper $U triangular matrices

- finding the unknown quantities in a linear system of equations

$$A = LU$$

Use **row reduce** to find $L$ and $U$

**QR decomposition** : decompose a square/rectangular matrix into an **orthogonal** matrix $Q$ and upper triangular matrix $R$

**eigen analysis** : a $kxk$ square matrix $A$ is said to have an **eigenvalue** $\lambda$ with corresponding **eigenvector** $x$ where $x ≠ 0$ if $Ax = \lambda x$

- eigenvalues are scalars that satisfy $|A - \lambda I| = 0$
- eigenvectors are normalized such that length is 1 ($x^Tx = 1$)

Formal definition of eigen

$$A \overrightarrow{v} = \lambda \overrightarrow{v}$$

... says that when the eigenvector $\overrightarrow{v}$ receives a linear transformation $A$, it is the same as the eigenvector $\overrightarrow{v}$ being scaled by an eigenvalue $\lambda$


**positive definite matrix** : a square matrix where for any non-zero vector $x$, the result of $x^TAx$ is always positive

- all eigenvalues are positive

**positive semidefinite matrix** :  a less strict version of PDM (allows zeros)

**diagonal matrix** : stretches the axis

**orthogonal matrix** : rotates the axis

**symmetric matrix** : a matrix with some form of symmetry along the diagonal

**rectangle matrix** : not symmetric

**transpose** : convert matrix $A$ to $A^T$ by turning the rows into columns

- symmetric matrix : $S = S^T$
- orthogonal matrix : $Q^T = Q^{-1}$
    - the transpose of an orthogonal matrix means rotation in the reverse direction

**matrix decomposition** : discovering the matrices that multiply together to form a matrix

[**spectral decomposition**](https://www.youtube.com/watch?v=mhy-ZKSARxI) : whenever you have a symmetric matrix you can always unconditionally decompose it into a sequence of three simple matrices

$$S = Q \Lambda Q^T$$

... where

- $S$ : symmetric matrix
- $Q$ : orthogonal matrix
- $\Lambda$ : diagonal matrix

... where the 

- columns of matrix $Q$ are the normalized eigenvectors of $S$
- diagonal values of matrix $\Lambda$ is the eigenvalues of the eigenvectors from left to right

$$
\begin{bmatrix} S \end{bmatrix} = 
\begin{bmatrix} | && | && | && | \\ e_1 && e_2 && ... && e_n \\ | && | && | && | \end{bmatrix}
\begin{bmatrix} \lambda_1 \\ && \lambda_2 \\ && && ... \\ && && && \lambda_n \\ \end{bmatrix}
\begin{bmatrix} | && | && | && | \\ e_1 && e_2 && ... && e_n \\ | && | && | && | \end{bmatrix}^T
$$

**principle component analysis (PCA)** : reducing the dimensionality of data by keeping the most important information

Application of PCA :

1. Assume a dataset with $p$ features (columns) $X_1, X_2, ... , X_p$
2. Compute the covariance matrix $\Sigma$ that captures feature relationships that follows :

    - high correlation = redundant information (we want independent variables)
    - $e_j$ : eigenvector (direction in the dataset)
    - $\lambda_j$ : eigenvalue (variance of that direction)
    - $\lambda_1 ≥ \lambda_2 ≥ ... ≥ \lambda_p$ : order of variance importance

3. Project data onto principal components with 
    $$Y_j = Xe_j = e_{j1}X_1 + ... + e_{jp}X_p$$
    where we will keep the top $k$ components (ones that retain most of the variance in the data)

_Ex :_

Given the covariance matrix $\Sigma$

$$\Sigma = \begin{bmatrix} 1 & -2 & 0 \\ -2 & 5 & 0 \\ 0 & 0 & 2 \end{bmatrix}$$

... derived from dataset $X$ with $n$ observations and $3$ features

- the diagonals $[1,5,2]$ indicate the variances of each feature
- off diagonals $[-2, -2]$ indicate the covariances between feature pairs

Performing eigenvalue decomposition we get 

$$(\lambda_1 = 5.86, e_1 = [0.383, -0.924, 0])$$
$$(\lambda_2 = 2.00, e_2 = [0, 0, 1])$$
$$(\lambda_3 = 0.17, e_3 = [0.924, 0.383, 0])$$

From the information above we can deduce the following information :

1. The total variance = sum of eigenvalues = $5.86 + 2.00 + 0.17 = 8.03$
2. The proportion of total variance explained by **first** principle component :
$$\frac{\lambda_1}{Total\:Variance} = \frac{5.86}{8.03} = 0.73 $$
3. The principle components are

$$Y_1 = 0.383 * X_1 + (-0.924) * X_2 + 0 * X_3$$
$$Y_2 = X_3$$
$$Y_3 = 0.924 * X_1 + 0.383 * X_2 + 0 * X_3$$

Determining the number of principle components is often aided by :

- amount of total sample variance explained
- relative sizes of eigen values
- subject interpretation
- point of bend in scree plot

[**single value decomposition**](https://www.youtube.com/watch?v=vSczTbgc8Rc): any matrix regardless of symmetry/rank/shape can be unconditionally decomposed as

$$A = U\Sigma V^T$$

A rectangular matrix $A$ has the ability to transform a vector in the $m$ dimension to one in the $n$ dimension

- a linear transformation from $R_m$ to $R_n$

Any rectangular matrix $A$ of dimensions $nxm$, we can create 2 symmetric matrices $AA^T$ and $A^TA$

- $AA^T = S_L$ has dimensions $nxn$ 
    - $n$ perpendicular eigenvectors in $R_n$ (**left singular vector**)
- $A^TA = S_R$ has dimensions $mxm$
    - $m$ perpendicular eigenvectors in $R_m$ (**right singular vector**)

$S_L$ and $S_R$ are positive semi-definite matrices

- eigenvalues for each eigenvector is $\lambda_i ≥ 0$
- overlapping eigenvalues are identical 
    - $\lambda_{S_L1} = \lambda_{S_R1}$
    - $\lambda_{S_L2} = \lambda_{S_R2}$
    - any non overlap will be 0

The **singular values** of matrix $A$ is the **square root of overlapping eigenvalues**

Going back to the formula

$$A = S_L \Sigma S_R^T$$

... where $\Sigma$ is a matrix with the same dimension as $A$ where the diagonals are the singular values

$$\begin{bmatrix} \sigma_1 \\ && \sigma_2 \\ && && ... \\ \end{bmatrix}$$

$S_R^T$ : orthogonal matrix that applies rotation that rotates right singular vectors to standard basis

- singular vector with largest singular value aligns with x-axis
- singular vector with second largest singular value aligns with y-axis
- ...

$\Sigma$ : removes a dimension and scales axis by the singular value $\sigma$

$\S_L$ : rotates the standard basis to align with the left singular vectors