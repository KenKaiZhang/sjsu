# ISE 201 - Math Foundations for Decision and Data Science

## Types of Questions

- Descriptive: what is the summarized statistic of a dataset
- Exploratory: are there any patterns, trends or relationships within a dataset
- Inferential: what can be inferred about the population from the sample
- Predictive: what would the outcome be for a certain combination of features 
- Casual: how does changes in the levels of one factor causes changes in the other factor

## Understanding Data

|              | feature1 | feature2 | feature3 | featureN |
|--------------|----------|----------|----------|----------|
| observation1 |
| observation2 |
| observation3 |
| observation4 |

**features**: independent variable "x"

- **continous**: feature values that can take on **any** values within a range
    - compute summary statistics (mean, median, mode, variance, range, min, max) 
    - usable in mathematical transformations
- **categorical**: feature values represented in labels
    - compute summary statistics by categories
    - **nominal**: no order (`dog, cat, fish, etc...`)
    - **ordinal**: yes order (`high school < bachelor's < Master's < PhD`)

Continuous variables can be converted to categorical

**central tendency**: measure that identifies the center/typical value of a data set

- **arthmetic mean**: $\bar{x} = \frac{1}{n} \sum_{i=1}^{n} x_i$
- **median**: middle score or average of two middle scores when data is ranked
- **mode**: most frequent

**spread**: how data values are distributed

- **range**:$ \text{range} = \max(x) - \min(x) $
- **inter quartile range (IQR)**: $ IQR = \textit{UpperQuartile} - \textit{LowerQuartile} $
- **population variance**: 
$$ \sigma^2 = \frac{\sum_{i=1}^{n} (x_i - \mu)^2}{n} $$
- **population standard deviation**: 
$$ \sigma = \sqrt{\frac{\sum_{i=1}^{n} (x_i - \mu)^2}{n}} $$
- **sample variance**:
$$ s^2 = \frac{\sum_{i=1}^{n} (x_i - \bar{x})^2}{n - 1} $$

Population variance is for analyzing entire dataset and sample variance is for the subset

**relationship**: the amount of influence one variable change has on another

- **covariance**: 
    - positive = variables increase together
    - negative = one variable increase = other variable decrease
    - provides only the relationship direction
$$cov(X, Y) = \frac{\sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})}{n-1}$$
- **correlation**:
    - measure both strength + direction
    - values between 1 and -1
    - 1 = perfect positive linear relationship
    - -1 = perfect negative linear relationship
    - 0 = no relationship
$$p =\frac{cov_{x,y}}{{s_xs_y}}$$

## Factorizations

**elimination** : reducing a square matrix $A$ to lower $L$ and upper $U triangular matrices

- finding the unknown quantities in a linear system of equations

$$A = LU$$

Use **row reduce** to find $L$ and $U$

**QR decomposition** : decompose a square/rectangular matrix into an **orthogonal** matrix $Q$ and upper triangular matrix $R$

**eigen analysis** : a $kxk$ square matrix $A$ is said to have an **eigenvalue** $\lambda$ with corresponding **eigenvector** $x$ where $x ≠ 0$ if $Ax = \lambda x$

- eigenvalues are scalars that satisfy $|A - \lambda I| = 0$
- eigenvectors are normalized such that length is 1 ($x^Tx = 1$)

Formal definition of eigen

$$A \overrightarrow{v} = \lambda \overrightarrow{v}$$

... says that when the eigenvector $\overrightarrow{v}$ receives a linear transformation $A$, it is the same as the eigenvector $\overrightarrow{v}$ being scaled by an eigenvalue $\lambda$


**positive definite matrix** : a square matrix where for any non-zero vector $x$, the result of $x^TAx$ is always positive

- all eigenvalues are positive

**positive semidefinite matrix** :  a less strict version of PDM (allows zeros)

**diagonal matrix** : stretches the axis

**orthogonal matrix** : rotates the axis

**symmetric matrix** : a matrix with some form of symmetry along the diagonal

**rectangle matrix** : not symmetric

**transpose** : convert matrix $A$ to $A^T$ by turning the rows into columns

- symmetric matrix : $S = S^T$
- orthogonal matrix : $Q^T = Q^{-1}$
    - the transpose of an orthogonal matrix means rotation in the reverse direction

**matrix decomposition** : discovering the matrices that multiply together to form a matrix

[**spectral decomposition**](https://www.youtube.com/watch?v=mhy-ZKSARxI) : whenever you have a symmetric matrix you can always unconditionally decompose it into a sequence of three simple matrices

$$S = Q \Lambda Q^T$$

... where

- $S$ : symmetric matrix
- $Q$ : orthogonal matrix
- $\Lambda$ : diagonal matrix

... where the 

- columns of matrix $Q$ are the normalized eigenvectors of $S$
- diagonal values of matrix $\Lambda$ is the eigenvalues of the eigenvectors from left to right

$$
\begin{bmatrix} S \end{bmatrix} = 
\begin{bmatrix} | && | && | && | \\ e_1 && e_2 && ... && e_n \\ | && | && | && | \end{bmatrix}
\begin{bmatrix} \lambda_1 \\ && \lambda_2 \\ && && ... \\ && && && \lambda_n \\ \end{bmatrix}
\begin{bmatrix} | && | && | && | \\ e_1 && e_2 && ... && e_n \\ | && | && | && | \end{bmatrix}^T
$$

**principle component analysis (PCA)** : reducing the dimensionality of data by keeping the most important information

Application of PCA :

1. Assume a dataset with $p$ features (columns) $X_1, X_2, ... , X_p$
2. Compute the covariance matrix $\Sigma$ that captures feature relationships that follows :

    - high correlation = redundant information (we want independent variables)
    - $e_j$ : eigenvector (direction in the dataset)
    - $\lambda_j$ : eigenvalue (variance of that direction)
    - $\lambda_1 ≥ \lambda_2 ≥ ... ≥ \lambda_p$ : order of variance importance

3. Project data onto principal components with 
    $$Y_j = Xe_j = e_{j1}X_1 + ... + e_{jp}X_p$$
    where we will keep the top $k$ components (ones that retain most of the variance in the data)

_Ex :_

Given the covariance matrix $\Sigma$

$$\Sigma = \begin{bmatrix} 1 & -2 & 0 \\ -2 & 5 & 0 \\ 0 & 0 & 2 \end{bmatrix}$$

... derived from dataset $X$ with $n$ observations and $3$ features

- the diagonals $[1,5,2]$ indicate the variances of each feature
- off diagonals $[-2, -2]$ indicate the covariances between feature pairs

Performing eigenvalue decomposition we get 

$$(\lambda_1 = 5.86, e_1 = [0.383, -0.924, 0])$$
$$(\lambda_2 = 2.00, e_2 = [0, 0, 1])$$
$$(\lambda_3 = 0.17, e_3 = [0.924, 0.383, 0])$$

From the information above we can deduce the following information :

1. The total variance = sum of eigenvalues = $5.86 + 2.00 + 0.17 = 8.03$
2. The proportion of total variance explained by **first** principle component :
$$\frac{\lambda_1}{Total\:Variance} = \frac{5.86}{8.03} = 0.73 $$
3. The principle components are

$$Y_1 = 0.383 * X_1 + (-0.924) * X_2 + 0 * X_3$$
$$Y_2 = X_3$$
$$Y_3 = 0.924 * X_1 + 0.383 * X_2 + 0 * X_3$$

Determining the number of principle components is often aided by :

- amount of total sample variance explained
- relative sizes of eigen values
- subject interpretation
- point of bend in scree plot

[**single value decomposition**](https://www.youtube.com/watch?v=vSczTbgc8Rc): any matrix regardless of symmetry/rank/shape can be unconditionally decomposed as

$$A = U\Sigma V^T$$

A rectangular matrix $A$ has the ability to transform a vector in the $m$ dimension to one in the $n$ dimension

- a linear transformation from $R_m$ to $R_n$

Any rectangular matrix $A$ of dimensions $nxm$, we can create 2 symmetric matrices $AA^T$ and $A^TA$

- $AA^T = S_L$ has dimensions $nxn$ 
    - $n$ perpendicular eigenvectors in $R_n$ (**left singular vector**)
- $A^TA = S_R$ has dimensions $mxm$
    - $m$ perpendicular eigenvectors in $R_m$ (**right singular vector**)

$S_L$ and $S_R$ are positive semi-definite matrices

- eigenvalues for each eigenvector is $\lambda_i ≥ 0$
- overlapping eigenvalues are identical 
    - $\lambda_{S_L1} = \lambda_{S_R1}$
    - $\lambda_{S_L2} = \lambda_{S_R2}$
    - any non overlap will be 0

The **singular values** of matrix $A$ is the **square root of overlapping eigenvalues**

Going back to the formula

$$A = S_L \Sigma S_R^T$$

... where $\Sigma$ is a matrix with the same dimension as $A$ where the diagonals are the singular values

$$\begin{bmatrix} \sigma_1 \\ && \sigma_2 \\ && && ... \\ \end{bmatrix}$$

$S_R^T$ : orthogonal matrix that applies rotation that rotates right singular vectors to standard basis

- singular vector with largest singular value aligns with x-axis
- singular vector with second largest singular value aligns with y-axis
- ...

$\Sigma$ : removes a dimension and scales axis by the singular value $\sigma$

$\S_L$ : rotates the standard basis to align with the left singular vectors

## Probability 

The likelihood that the outcome will occur when the uncertainty is resolved

- Value between 0 (impossible) and 1 (certainty)

**event** : collection of outcomes

**probability of an event** : sum of the probabilities of the outcomes

--- 

**Fundamental Laws of Probability** 

1. Probability of an event is between 0 and 1
2. If $A$ and $B$ are **mutually exclusive**
    - Sum Rule : $P(A\,or\,B) = P(A \cup  B) = P(A) + P(B)$
3. If $A$ and $B$ are two events
    - Product Rule : $P(A\,and\,B) = P(A \cap B) = P(A) \times P(B)$
    - Sum Rule : $P(A \cup B) = P(A) + P(B) - P(A\,and\, B)$
4. Conditional Probability : if $A$ and $B$ are two events, then 
    $$P(A|B) = \frac{P(A\,and\,B)}{P(B)} = \frac{P(A\cap B)}{P(B)}$$
    ... where
    $$P(A\,and\,B) = P(A|B) \times P(B)$$
5. If $A$ and $B$ are **independent** : $P(A|B) = P(A)$
6. If $A$ and $B$ are independent then $P(A\,and\,B) = P(A) \times P(B)$

---

**Bayes Theorem**

How to update probabilities based on new information

$$P(A|B) = \frac{P(B|A)P(A)}{P(B)}$$

... where

- $P(A|B)$ = probability of $A$ happening given $B$ happened (**posterior probability**)
- $P(B|A)$ = probability of $B$ happening given $A$ happened (**likelihood**)
- $P(A)$ = probability of $A$ happening on its own (**prior probability**)
- $P(B)$ = probability of $B$ happening on its own (**total probability**)

---

**Law of Total Probability**

Calculates the probability of an event by considering all possible ways that an event can occur

If events $B_1, B_2, ... ,B_n$ form a partition of the sample space

- mutually exclusive
- covers all possibilities

... then for any event $A$ 

$$P(A) = \sum^n_{i=1}P(A|B_i)P(B_i)$$

_ex_ : Assume a hospital with two patients 

- outpatients (70%)
- inpatients (30%)

... where the flu rates of each group is 

- outpatient (5%)
- inpatient (20%)

... then the overall probability that a chosen patient has the flu $P(A)$ is

$$P(A) = P(A|B_1)P(B_1) + P(A|B_2)P(B_2)$$
$$= 0.05 \times 0.7 + 0.2 \times 0.3$$
$$=0.035 + 0.06 = 0.095 = 9.5\%$$

---

**Counting**

Finding the number of outcomes from a sample space

Multiplication Rule : perform a task that happens over $k$ steps with $n$ choices each 

$$n_1 \times n_2 \times ... \times n_k$$

_ex_ : Choosing an outfit between 3 shirts, 2 pants, and 4 shoes : $3 \times 2 \times 4 = 24$ ways


Permutations : number of ways to arrange objects in order

- Entire Set : $n$ distinct objects 
$$n! = n \times (n-1) \times (n-2) \times ... \times 1$$
- Subset : $r$ objects from $n$ (order matters)
$$P^n_r = \frac{n!}{(n-r)!}$$
- Similar : repeating objects in the set
$$\frac{n!}{n_1!n_2!...n_r!}$$
... where $n_1!n_2! ... n_r!$ are the counts of repeated elements

Combinations : choosing $r$ objects from $n$ without regard of order
$$C^n_r = \frac{n!}{r!(n-r)!}$$

_ex_ : Choosing 3 students from 5

- $C^5_3 = \frac{5!}{3!(5-3)!} = \frac{120}{6(2)} = 10$

---

**random variables** : uncertain outcome of a random experiment or a probability model

**discrete random variable** : a variable that can take on a countable number of distinct values where each value has a certain probability of occurring

_ex_ : Number of heads when flipping a coin 3 times $X \in \{0,1,2,3\}$

**probability distribution** : how probabilities are assigned to different values of a random variable

_ex_ : The probability distribution for a fair dice 
$$P(X = x) = \frac{1}{6} \qquad x \in \{1,2,3,4,5,6\}$$

**probability mass function (PMF)** : function that gives the probability of each possible value of a discrete variable

_ex_ : PMF will give each outcome of a dice the probability of $\frac{1}{6}$ which satisfies

- $P(X = x_i) ≥ 0 $ for a ll $x_i$
- Sum of probabilities is $1$

**cumulative probability distribution function (CDF)** : cumulative probability that a discrete random variable will take a value less than or equal to a particular value $x$

$$F(x) = P(X ≤ x)$$

... where 

- $F(X)$ : CDF
- $P(X ≤ x)$ : the probability that the random variable $X$ takes a values $≤ x$

_ex_ : For a dice, the CDF for $X = 3$ would be

$$F(3) = P(X ≤ 3) = P(X = 1) + P(X = 2) + P(X = 3)$$
$$= 3 \times \frac{1}{6} $$
$$= 0.5$$

... therefore the cumulative probability a dice will roll a 1, 2, or 3 is $0.5$

---

**Summary Measures**

For random variable $X$

_ex_ will use a dice example where $X$ represents the outcome of rolling the dice

Mean / Average / Expected Value :

$$\mu_X = E(X) = \sum^n_{i=1}P(X = x_i) \times x_i = \sum^n_{i=1}p_i(x_i - \mu_X)^2$$

_ex_ : 

$$E(X) = \sum^6_{i=1}P(X=x_i) \times x_i$$
$$= \frac{1}{6} \times (1+2+3+4+5+6)$$
$$= 3.5$$

Variance :

$$\sigma^2_X = Var(X) = \sum^6_{i=1}P(X=x_i) \times (x_i-\mu_X)^2 = \sum^n_{i=1}p_i(x_i - \mu_x)^2$$


_ex_  :

$$Var(X) = \sum^n_{i=1}P(X=x_i) \times (x_i-\mu_X)^2$$
$$= \frac{1}{6} \times [(1-3.5)^2 + (2-3.5)^2 + (3-3.5)^2 + (4-3.5)^2 + (5-3.5)^2 + (6-3.5)^2 ]$$
$$= \frac{1}{6} \times [6.25 + 2.25 + 0.25 + 0.25 + 2.25 + 6.25]$$
$$= 2.9167$$

Standard Deviation : 

$$\sigma_x = \sqrt{Var(X)}$$

_ex_ : $\sqrt{Var(X)} = 1.7078$

---

**binomial distribution** : probability experiments of a sequence of $n$ independent trials where each trial has exactly two possible outcomes (Bernoulli trial)

- probability of **success** is $p$ which is the same per trial
- probability of **failure** is $1-p$ which is the same per trial

$P(all\,trials\,are\,success) = P(X=n) = p^n$

$P(all\,trials\,are\,failures) = P(X=0) =(1 - p)^n$

$P(X = x)$ for $x - 1,2, ..., (n - 1)$ can happen multiple ways

- first $x$ trials are success and remaining $n-x$ are failures : $p^x(1-p)^{n-x}$
- first $n-x$ are failures and then last $x$ trials are successes

Ways $x$ successes will occur in $n$ trials is $\frac{n!}{x!(n-x)!}$

$$P(X = x) = \frac{n!}{x!(n-x)!}p^x(1-p)^{n-x}$$
$$E(X) = np$$
$$Var(X) = np(1-p)$$

Types of discrete distributions :

- uniform
- binomial
- geometric : number of trials conducted until a success is obtained
- negative binomial : number of trials conducted until $r$ successes are obtained
- hyper-geometric : a set of $N$ objects has $K$ successes or $(N-K)$ failures
- poisson : count of events that occur within an interval

**linear function of a random variable** : any function that can be written in $Y = aX + b$

- $E(Y) = a\mu_X + b$
- $Var(Y) = a^2Var(X)$

If $X$ and $Y$ are two random variables with $\mu_X$ and $\mu_Y$,

$$Cov(X,Y) = \sum^n_{i=1}p_i(x_i - \mu_X)(y_i - \mu_Y)$$

... measures how much the two variables vary together

$$Cor(X,Y) = \frac{Cov(X,Y)}{\sigma_X\sigma_Y}$$

- If $Cov(X,Y) = 0$, then $X$ and $Y$ are independent

**conditional probability density function** : describes how the distribution of $X$ changes when we know that $Y$ is fixed at some value

$$f_{Y|x}(y) = \frac{f_{XY}(x,y)}{f_X(x)}\quad for \quad f_X(x) > 0$$

**joint probability distribution** : the probability of 2 or more random variables occurring together

Two random variables $X$ adn $Y$ are independent if 

- $P(X = x, Y = y) = P(X = x) \times P(Y = y)$
- E(X * Y) = E(X) \times E(Y)$
- $Cov(X,Y) = 0$

## Continuous Random Variables

Takes on any numerical value within a range

**probability density function (PDF)** : likelihood of a continuous random variable taking on a specific value

- Total area under the function curve is $1$
- $P(a < X < b)$ = area under the curve between $a$ and $b$

**cumulative distribution function** : probability a random variable $X$ takes on a value less than or equal to $t$

$$F(t) = P(X ≤ t)$$

... where 

- $F(t)$ must be between 0 and 1
- $F(t)$ in an increasing function
- $P(X > t) = 1 - F(t)$
- $P(c ≤ X ≤ d) = F(d) - F(c)$

**normal distribution** : bell-shape curve

For a normal distribution with $\mu$ and $\sigma$, the PDF of $X$ is

$$f(x) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x - \mu)^2}{2\sigma^2}}$$

... where $X$ is more likely to take values closer to $\mu$ (mean) than far from it

**standard normal** : normal distribution where $\mu = 0$ and $\sigma = 1$

- denoted as $Z$ ~ $N(0,1)$ where $Z=\frac{X -\mu}{\sigma}$

---

**Central Limit Theorem**

Regardless of the shape of the population distribution, the distribution of the sample mean (or sum) will tend to be approximately normal (bell-shaped) as the sample size increases, provided the samples are independent and identically distributed

Suppose $X_1, X_2, ... , X_n$ are independent and identically distributed random variables

Let $S_n = \sum^n_{i=1}X_i$

if $n$ is large ($n > 30$), then $S_n$ is approximately Normally distributed with mean $n\mu$ and standard deviation $\sigma\sqrt{n}$

Let $\bar{X}=\frac{\sum^n_{i=1}X_i}{n}$

if $n$ is large, then $\bar{X}$ is approximately Normally distributed with mean $\mu$ and standard deviation $\frac{\sigma}{\sqrt{n}}$

---

Common continuous distributions

- Lognormal : natural log of $X$ is normally distributed
- exponential : time between events from a Poisson process
- Weibull : time until failure
- beta : continuous values are bounded over a finite range

**Empirical Cumulative Distribution Function** : step function that estimates the CDF of a dataset

Given a dataset of $n$ observations, the EDCF is

$$F_n(x)=\frac{number\,of\,observations≤x}{n}=\frac{1}{n}\sum^n_{i=1}1_{X_i≤x}$$